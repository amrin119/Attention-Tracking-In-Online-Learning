 
CODES

Training Codes
pip install keras_metrics
from google.colab import drive
drive.mount('/content/drive')
import keras
import pandas as pd
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model 
from keras.layers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
 
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
import keras.metrics as km
 
 
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
 
 
import matplotlib.pyplot as plt    # for plotting the images
%matplotlib inline
from keras.preprocessing import image 
from tqdm import tqdm
import pickle
 
Data preparation
#Declaring constants
data_dir = "/content/drive/MyDrive/attention_tracking/dataset"
img_height , img_width = 64, 64
#seq_len = 80
seq_len = 25
 
classes = ["writing","mobile"]

Extraction of frames from videos
 
def create_data(input_dir):
    
    X = []
    Y = []
    classes_list = os.listdir(input_dir)
    for c in tqdm(classes_list):
        files_list = os.listdir(os.path.join(input_dir, c))
        for f in files_list:
            print(f)
            video_location = os.path.join(os.path.join(input_dir, c), f) #it will pick a video location inside a class(leaving,looking etc)
        
            vs = cv2.VideoCapture(video_location)
            while True:
                count = 0
                defect_flag = 0
                frames_list = []
                while count < seq_len: 
                    grabbed, frame = vs.read() 
                    if grabbed:
                        frame = cv2.resize(frame, (img_height, img_width))
                        #plt.imshow(image,cmap = plt.cm.gray )
                        #plt.show()
                        frames_list.append(frame)
                        count += 1
                    else:
                        #print("Defected frame")
                        defect_flag = 1
                        break
                #print(len(frames_list))
                if len(frames_list) == seq_len:
                    #print("yes")
                    X.append(frames_list)
                    y = [0]*len(classes)
                    y[classes.index(c)] = 1
                    Y.append(y)
                if defect_flag == 1:
                    break
    X = np.asarray(X)
    Y = np.asarray(Y)
    return X,Y

X, Y = create_data(data_dir)
print(Y)
print(Y)

print("Shape of X {}. Shape of Y {}".format(X.shape,Y.shape))

Store the value of X and Y
#storing the array into byte file
pickle.dump( X, open( "/content/drive/MyDrive/attention_tracking/X & Y/X", "wb" ) )
pickle.dump( Y, open( "/content/drive/MyDrive/attention_tracking/X & Y/Y", "wb" ) )

Load the value of X and Y
#reading the array from the byte file
X = pickle.load(open("/content/drive/MyDrive/attention_tracking/X & Y/X",'rb') )
Y = pickle.load(open("/content/drive/MyDrive/attention_tracking/X & Y/Y",'rb') )

Number of samples in each class
dic = {"writing":0,"mobile":0}
for i in range(len(Y)):
    for j in range(len(Y[i])):
        if Y[i][j] == 1:
            if j == 0:
                dic["writing"] += 1
            else:
                dic["mobile"] += 1
print("samples in writing {}. samples in mobile {}.".format(dic["writing"],dic["mobile"]))

print(X[0])
print(Y)

Training the model
Before starting training we need to split the data into train and test dataset.
Train Test Split
#stratify=Y
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, shuffle=True, random_state=0)

print("{}{:^30}".format("Length of y_train","Length of y_test"))
print("{}{:^60}".format(len(y_train),len(y_test)))
print("{}{:^30}".format("Length of X_train","Length of X_test"))
print("{}{:^60}".format(X_train.shape[0],X_test.shape[0]))

ConvLSTM based model design
model = Sequential()
model.add(ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last", input_shape = (seq_len, img_height, img_width, 3)))
#model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(256, activation="relu"))
#model.add(Dropout(0.2))
model.add(Dense(256, activation="relu"))
#model.add(Dropout(0.3))
model.add(Dense(2, activation = "softmax"))

model.summary()

Now we compile the model. We can also try with different optimizers and hyper-parameters settings for better results.

Values of confusion matrix
metrics = [
    keras.metrics.TruePositives(name="tp"),
    keras.metrics.TrueNegatives(name="tn"),
    keras.metrics.FalseNegatives(name="fn"),
    keras.metrics.FalsePositives(name="fp"),
]

#opt = keras.optimizers.SGD(lr=0.001)
opt = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy",metrics])
 
Weâ€™ll use the Early Stopping to avoid over-fitting.
earlystop = EarlyStopping(patience=8)
callbacks = [earlystop]

Saving the weights of best model.
#defining a function to save the weights of best model
#save_weights_only=True, only stores the best weights
#checkpoint_filepath = '/tmp/checkpoint'
#filepath=checkpoint_filepath,
import os 
lis = os.listdir("/content/drive/MyDrive/attention_tracking/weight")
length = len(lis)
 
from keras.callbacks import ModelCheckpoint
mcp_save = ModelCheckpoint('/content/drive/MyDrive/attention_tracking/weight/weights_of_test1'+'_run'+str(length+1)+'.hdf5', save_best_only=True, monitor='val_loss', mode='min')

Training the model
Epochs :- How many times our model will go through data
Batch size :- How much amount of data at once you wanna pass through the model

# if shuffle = False, then we get higher accuracy
history = model.fit(x = X_train, y = y_train, epochs=10, batch_size = 8, shuffle=True, validation_data=(X_test,y_test), callbacks=[callbacks,mcp_save])
#history = model.fit(x = X, y = Y, epochs=10, batch_size = 8, shuffle=True, validation_split=0.2, callbacks=[mcp_save],)
Storing all the losses and accuracies
Store CSV file
df = pd.DataFrame()
label = list(history.history.keys())
for i in range(len(label)):
  df[label[i]] = history.history[label[i]]
df.to_csv("/content/drive/MyDrive/attention_tracking/score/accuracy_loss.csv")
print(label)

Read CSV file & storing accuracy,loss,values of confusion matrix
read_df = pd.read_csv("/content/drive/MyDrive/attention_tracking/score/accuracy_loss.csv")
#df = pd.DataFrame({"ID":read_df.ID,"Name":read_df.Name})
loss = np.array(read_df.loss)
val_loss = np.array(read_df.val_loss)
accuracy = np.array(read_df.accuracy)
val_accuracy = np.array(read_df.val_accuracy)
#confusion matrix value for training
tp = np.array(read_df.tp)
fp = np.array(read_df.fp)
fn = np.array(read_df.fn)
tn = np.array(read_df.tn)
 
#confusion matrix value for validation
val_tp = np.array(read_df.val_tp)
val_fp = np.array(read_df.val_fp)
val_fn = np.array(read_df.val_fn)
val_tn = np.array(read_df.val_tn)
print(accuracy,val_accuracy)

read_df.head()

Accuracy = np.mean(accuracy)
Val_accuracy = np.mean(val_accuracy)
print("{}{:^30}".format("Training Accuracy","Validation Accuracy"))
print("{:0.2f}{:^18}{:0.2f}".format(Accuracy,'',Val_accuracy))


import keras
import pandas as pd
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model 
from keras.layers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
 
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
import keras.metrics as km
 
 
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
 
 
import matplotlib.pyplot as plt    # for plotting the images
%matplotlib inline
from keras.preprocessing import image 
from tqdm import tqdm
import pickle

# import the necessary packages
from keras.models import load_model
from collections import deque
import numpy as np
import argparse
import pickle
import cv2

img_height , img_width = 64, 64
seq_len = 25
model = Sequential()
model.add(ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, data_format = "channels_last", input_shape = (seq_len,img_height, img_width, 3)))
#model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(256, activation="relu"))
#model.add(Dropout(0.2))
model.add(Dense(256, activation="relu"))
#model.add(Dropout(0.3))
model.add(Dense(2, activation = "softmax"))

#loading the trained weights
model.load_weights('weight/weights_of_test1_run1.hdf5')


metrics = [
    keras.metrics.TruePositives(name="tp"),
    keras.metrics.TrueNegatives(name="tn"),
    keras.metrics.FalseNegatives(name="fn"),
    keras.metrics.FalsePositives(name="fp"),
]

#opt = keras.optimizers.SGD(lr=0.001)
opt = keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=["accuracy",metrics])



args = {
          'input':r'input/test04.mp4',
          'output':r'output/test04.avi',
          'size':128,
        }

store_tag = [] #it will store tag for per 80 frames
save_frames = [] # store all frames of sequences
classes = ["writting","usingPhone"]

vs = cv2.VideoCapture(args["input"])



writer = None
(W, H) = (None, None)
defect_flag = 0

while True:
    count = 0
    defect_flag = 0
    frames_list = []
    X = []
    #saving sequence frames
    while count < seq_len: 
        grabbed, frame = vs.read() 
        count += 1
        if grabbed:
            frame = cv2.resize(frame, (img_height, img_width))
            #plt.imshow(image,cmap = plt.cm.gray )
            #plt.show()
            frames_list.append(frame)
        else:
            #print("Defected frame")
            defect_flag = 1
            break
            
    #print(len(frames_list))
    #predicting each frames of the sequence
    if len(frames_list) == seq_len:
        for frm in frames_list:
            save_frames.append(frm)
        #print("yes")
        X.append(frames_list)
        X = np.asarray(X)
        y_pred = model.predict(X)
        print(y_pred)
        #val = np.array(y_pred).mean(axis=0)
        val = np.argmax(y_pred)
        #print(val)
        #print(classes[val])
        store_tag.append(classes[val])

        
    if defect_flag == 1:
        break



frames_list = []
classes = ["writting","phone"]

Q = deque(maxlen=128)

vs = cv2.VideoCapture(args["input"])

writer = None
(W, H) = (None, None)
count = 0
i = 0
index = 0
text = ""
while True:
    (grabbed, frame) = vs.read()
    if not grabbed:
        break

    if W is None or H is None:
        (H, W) = frame.shape[:2]

    output = frame.copy()
    
   
    if (i%25==0) and (i >= 30):
        if index < len(store_tag)-1:
            index += 1
        text = "Activity: {}".format(store_tag[index])
    
    cv2.putText(output, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (255, 0, 0), 5)

    if writer is None:
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")
        writer = cv2.VideoWriter(args["output"], fourcc, 30,
            (W, H), True)

    writer.write(output)
    print(writer) 
    #cv2.resizeWindow("output", 128,128)
    output = cv2.resize(output, (550,550)) 
    cv2.imshow("Output", output)
    i += 1
    #plt.imshow(output,cmap = plt.cm.gray )
    #plt.show()
    key = cv2.waitKey(5) & 0xFF
    
    if key == ord("q"):
        break

print("[INFO] cleaning up...")
print(counter)
writer.release()
vs.release()
